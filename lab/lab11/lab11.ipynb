{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab11.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 11 Decision Tree, Random Forest and PCA#\n",
    "\n",
    "This lab will have two disjoint parts. \n",
    "\n",
    "In the first part, we will have you train a multi-class classifier with three different models (one-vs-rest logistic regression, decision tree, random forest) and compare the accuracies and decision boundaries created by each.\n",
    "\n",
    "In the second part, we will have you perform Principal Component Analysis on Iris data.\n",
    "\n",
    "**Note: This lab is a bit longer than the previous labs. You should start early!**\n",
    "\n",
    "### Due Date\n",
    "\n",
    "This assignment is due at Monday, May 4th at 11:59 pm PST.\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import tree\n",
    "# ignore the warning you might get from importing ensemble from sklearn\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I â€“ Classification\n",
    "\n",
    "Let's start with classification! We'll now be looking at a dataset of per-game stats for all NBA players in the 2018-19 season. This dataset comes from basketball-reference.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_data = pd.read_csv(\"nba18-19.csv\")\n",
    "nba_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to predict a player's position given several other features. The 5 positions in basketball are PG, SG, SF, PF, and C (which stand for point guard, shooting guard, small forward, power forward, and center). This information is contained in the `Pos` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_data['Pos'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could set out to try and perform 5-class classification, the results (and visualizations) are slightly more interesting if we try and categorize players into 1 of 3 categories: **guard**, **forward**, and **center**. The below code will take the `Pos` column of our dataframe and use it to create `Pos3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_position(pos):\n",
    "    if 'F' in pos:\n",
    "        return 'F'\n",
    "    elif 'G' in pos:\n",
    "        return 'G'\n",
    "    return 'C'\n",
    "\n",
    "nba_data['Pos3'] = nba_data['Pos'].apply(basic_position)\n",
    "nba_data['Pos3'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, since there are **many** players in the NBA (in the 2018-19 season there were 530 unique players), our visualizations can tend to get noisy and messy. Let's restrict our data to only contain rows for players that averaged 10 or more points per game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_data = nba_data[nba_data['PTS'] > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at a scatterplot of Rebounds (`TRB`) vs. Assists (`AST`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = nba_data, x = 'AST', y = 'TRB', hue = 'Pos3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, when using just rebounds and assists as our features, we see pretty decent cluster separation. That is, Centers, Forwards, and Guards appear in different regions of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-Rest Logistic Regression\n",
    "\n",
    "We only discussed binary logistic regression in class, but there is a natural extension to binary logistic regression called one-vs-rest logistic regression for multiclass classification. In essence, one-vs-rest logistic regression simply builds one binary logistic regression classifier for each of the $N$ classes (in this scenario $N = 3$). We then predict the class corresponding to the classifier that gives the highest probability among the $N$ classes.\n",
    "\n",
    "Before using logistic regression, let's first split `nba_data` into a training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_train, nba_test = train_test_split(nba_data, test_size=0.25, random_state=100)\n",
    "nba_train = nba_train.sort_values(by='Pos')\n",
    "nba_test = nba_test.sort_values(by='Pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1a\n",
    "\n",
    "In the cell below, set `logistic_regression_model` to be a one-vs-rest logistic regression model. Then, fit that model using the `AST` and `TRB` columns (in that order) from `nba_train` as our features, and `Pos3` as our response variable.\n",
    "\n",
    "Remember, [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) has already been imported for you. There is an optional parameter **`multi_class`** you need to specify in order to make your model a multi-class one-vs-rest classifier. See the documentation for more details.\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see our classifier in action, we can use `logistic_regression_model.predict` and see what it outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_train['Predicted (OVRLR) Pos3'] = logistic_regression_model.predict(nba_train[['AST', 'TRB']])\n",
    "nba_train[['AST', 'TRB', 'Pos3', 'Predicted (OVRLR) Pos3']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model does decently well here, as you can see visually above. Below, we compute the training accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_training_accuracy = logistic_regression_model.score(nba_train[['AST', 'TRB']], nba_train['Pos3'])\n",
    "lr_training_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the test accuracy as well by looking at `nba_test` instead of `nba_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_test_accuracy = logistic_regression_model.score(nba_test[['AST', 'TRB']], nba_test['Pos3'])\n",
    "lr_test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's draw the decision boundary for this logistic regression classifier, and see how the classifier performs on both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns_cmap = ListedColormap(np.array(sns.color_palette())[0:3, :])\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(0, 12, 0.02), np.arange(0, 16, 0.02))\n",
    "Z_string = logistic_regression_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "sns.scatterplot(data = nba_train, x = 'AST', y = 'TRB', hue = 'Pos3')\n",
    "plt.title('Logistic Regression on nba_train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_cmap = ListedColormap(np.array(sns.color_palette())[0:3, :])\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(0, 12, 0.02), np.arange(0, 16, 0.02))\n",
    "Z_string = logistic_regression_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "sns.scatterplot(data = nba_test, x = 'AST', y = 'TRB', hue = 'Pos3')\n",
    "plt.title('Logistic Regression on nba_test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our one-vs-rest logistic regression was able to find a linear decision boundary between the three classes. It generally classifies centers as players with a lot of rebounds, forwards as players with a medium number of rebounds and a low number of assists, and guards as players with a low number of rebounds. \n",
    "\n",
    "Note: In practice we would use many more features â€“ we only used 2 here just so that we could visualize the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "### Question 1b\n",
    "\n",
    "Let's now create a decision tree classifier on the same training data `nba_train`, and look at the resulting decision boundary. \n",
    "\n",
    "In the following cell, first, use [`tree.DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) to fit a model using the same features and response as above, and call this model `decision_tree_model`. Set the `random_state` parameter to 42.\n",
    "\n",
    "**Hint:** Your code will be mostly be the same as question 1a.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree_model = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's draw the decision boundary for this decision tree classifier, and see how the classifier performs on both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(0, 12, 0.02), np.arange(0, 16, 0.02))\n",
    "Z_string = decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "sns.scatterplot(data = nba_train, x = 'AST', y = 'TRB', hue = 'Pos3')\n",
    "plt.title('Decision Tree on nba_train');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(0, 12, 0.02), np.arange(0, 16, 0.02))\n",
    "Z_string = decision_tree_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "sns.scatterplot(data = nba_test, x = 'AST', y = 'TRB', hue = 'Pos3')\n",
    "plt.title('Decision Tree on nba_test');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1c\n",
    "\n",
    "Set `dt_training_accuracy` to be the training accuracy of the decision tree model and `dt_test_accuracy` to be the test accuracy.\n",
    "\n",
    "**Hint:** If you're failing this test, make sure you have the correct `random_state` parameter to `tree.DecisionTreeClassifier` in question 2b.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_training_accuracy = ...\n",
    "dt_test_accuracy = ...\n",
    "dt_training_accuracy, dt_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "### Question 1d\n",
    "\n",
    "Let's now create a random forest classifier on the same training data `nba_train` and look at the resulting decision boundary. \n",
    "\n",
    "In the following cell, use [`ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to fit a model using the same features and response as above, and call this model `random_forest_model`. Use 20 trees in your random forest classifier, and set the `random_state` parameter to 42.\n",
    "\n",
    "**Hint:** Your code for both parts will be mostly the same as questions 1a and 1b.\n",
    "\n",
    "**Hint:** Look at the `n_estimators` parameter of `ensemble.RandomForestClassifier`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1d\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's draw the decision boundary for this random forest classifier, and see how the classifier performs on both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(0, 12, 0.02), np.arange(0, 16, 0.02))\n",
    "Z_string = random_forest_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "sns.scatterplot(data = nba_train, x = 'AST', y = 'TRB', hue = 'Pos3')\n",
    "plt.title('Random Forest on nba_train');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(0, 12, 0.02), np.arange(0, 16, 0.02))\n",
    "Z_string = random_forest_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "categories, Z_int = np.unique(Z_string, return_inverse = True)\n",
    "Z_int = Z_int.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z_int, cmap = sns_cmap)\n",
    "sns.scatterplot(data = nba_test, x = 'AST', y = 'TRB', hue = 'Pos3')\n",
    "plt.title('Random Forest on nba_test');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1e\n",
    "\n",
    "Set `rf_train_accuracy` to be the training accuracy of the random forest model and `rf_test_accuracy` to be the test accuracy.\n",
    "\n",
    "**Hint:** If you're failing this test, make sure you have the correct parameters to `ensemble.RandomForestClassifier` in question 1d.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1e\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_accuracy = ...\n",
    "rf_test_accuracy = ...\n",
    "rf_train_accuracy, rf_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1f\n",
    "\n",
    "Looking at the three models you created (multiclass one-vs-rest logistic regression, decision tree, random forest), which model performed the best on the training set, and which model performed the best on the test set? How are the training and test accuracy related for the three models, and how do the decision boundaries generated for each of the three models relate to the model's performance?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1f\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II PCA\n",
    "\n",
    "In this part of the lab, we will perform Dimensionality Reduction using PCA.\n",
    "\n",
    "To begin, run the following cell to load the dataset into this notebook. \n",
    "* `iris_features` will contain a numpy array of 4 attributes for 150 different plants (shape 150 x 4). \n",
    "* `iris_target` will contain the class of each plant. There are 3 classes of plants in the dataset: Iris-Setosa, Iris-Versicolour, and Iris-Virginica. The class names will be stored in `iris_target_names`.\n",
    "* `iris_feature_names` will be a list of 4 names, one for each attribute in `iris_features`. \n",
    "\n",
    "Additional information on the dataset will be included in the description printed at the end of the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris() # Loading the dataset\n",
    "\n",
    "# Unpacking the data into arrays\n",
    "iris_features = iris_data['data']\n",
    "iris_target = iris_data['target']\n",
    "iris_feature_names = iris_data['feature_names']\n",
    "iris_target_names = iris_data['target_names']\n",
    "\n",
    "# Convert iris_target to string labels instead of int labels currently (0, 1, 2) for the classes\n",
    "iris_target = iris_target_names[iris_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produces scatter plots of our iris features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.suptitle(\"Scatter Matrix of Iris Features\")\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "for i in range(1, 4):\n",
    "    for j in range(i):\n",
    "        plt.subplot(3, 3, i+3*j)\n",
    "        sns.scatterplot(iris_features[:, i], iris_features[:, j], hue=iris_target)\n",
    "        plt.xlabel(iris_feature_names[i])\n",
    "        plt.ylabel(iris_feature_names[j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2a\n",
    "\n",
    "To apply PCA, we will first need to \"center\" the data so that the mean of each feature is 0. \n",
    "\n",
    "Compute the columnwise mean of `iris_features` in the cell below and store it in `iris_mean` (should be a numpy array of 4 means, 1 for each attribute). Then, subtract `iris_mean` from `iris_features`, and save the result in `features`.\n",
    "\n",
    "**Hints:** \n",
    "* Use `np.mean` or `np.average` to compute `iris_mean`, and pay attention to the `axis` argument.\n",
    "* If you are confused about how numpy deals with arithmetic operations between arrays of different shapes, see this note about [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for explanations/examples.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mean = ...\n",
    "features = ...\n",
    "iris_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2b\n",
    "\n",
    "As you may recall from lecture, PCA is a specific application of the singular value decomposition (SVD) for matrices. In the following cell, let's use the [`np.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) function compute the SVD of our `features`. Store the left singular vectors, singular values, and right singular vectors in `u`, `s`, and `vt` respectively.\n",
    "\n",
    "**Hint:** Set the `full_matrices` argument of `np.linalg.svd` to `False`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "u.shape, s, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn from the singular values in `s`? First, we can compute the total variance of the data by summing the squared singular values. We will later be able to use this value to determine the variance captured by a subset of our principal components.\n",
    "\n",
    "The cell below computes the total variance below by summing the square of `s`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_variance = np.sum(np.square(s)) / iris_features.shape[0]\n",
    "print(\"total_variance: {:.3f} should approximately equal the sum of feature variances: {:.3f}\"\n",
    "      .format(total_variance, np.sum(np.var(iris_features, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `total_variance` is equal to the sum of feature variances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2c\n",
    "\n",
    "Let's now use only the first two principal components to see what a 2D version of our iris data looks like.\n",
    "\n",
    "First, construct the 2D version of the iris data by matrix-multiplying our `features` by the first two right singular vectors in `v`. This will project the iris data down from a 4D subspace to a 2D subspace, and the first two right singular vectors are directions for the first two principal components.\n",
    "\n",
    "**Hints:**\n",
    "* To matrix multiply two numpy arrays, use @ or np.dot.\n",
    "* The first two right singular vectors in `v` will be the first two columns of `v`, or the first two rows of `vt` (transposed to be column vectors instead of row vectors). \n",
    "* Since we want to obtain a 2D version of our iris dataset, the shape of `iris_2d` should be (150, 2).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_2d = ...\n",
    "np.sum(iris_2d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to create the scatter plot of our 2D version of the iris data, `iris_2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(\"PC2 vs. PC1 for Iris Data\")\n",
    "plt.xlabel(\"Iris PC1\")\n",
    "plt.ylabel(\"Iris PC2\")\n",
    "sns.scatterplot(iris_2d[:, 0], iris_2d[:, 1], hue=iris_target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2d\n",
    "\n",
    "What do you observe about the plot above? If you were given a point in the subspace defined by PC1 and PC2, how well would you be able to classify the point as one of the three iris types?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2d\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What proportion of the total variance is accounted for when we project the iris data down to two dimensions? The cell below computes this quantity by dividing the sum of the first two squared singular values (also known as component scores) in `s` by the `total_variance` you calculated previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_variance = (np.sum(np.square(s[:2])) / iris_features.shape[0]) / total_variance\n",
    "two_dim_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the variances of the data are explained by its projection on two dimensions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2e\n",
    "\n",
    "As a last step, let's create a [scree plot](https://en.wikipedia.org/wiki/Scree_plot) to visualize the weight of each of each principal component. In the cell below, create a scree plot by plotting a line plot of the square of the singular values in `s` vs. the principal component number (1st, 2nd, 3rd, or 4th).\n",
    "\n",
    "<img src=\"scree.png\" width=\"400px\" />\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2e\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "ok.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
