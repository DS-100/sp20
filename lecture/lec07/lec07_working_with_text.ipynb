{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:15.363920Z",
     "start_time": "2018-02-02T15:15:14.337886Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import plotly.offline as py\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.set_config_file(offline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "# (Demo 1) Canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('county_and_state.csv') as f:\n",
    "    county_and_state = pd.read_csv(f)\n",
    "    \n",
    "with open('county_and_population.csv') as f:\n",
    "    county_and_pop = pd.read_csv(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we'd like to join these two tables. Unfortunately, we can't, because the strings representing the county names don't match, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before we can join them, we'll do what I call **canonicalization**.\n",
    "\n",
    "Canonicalization: A process for converting data that has more than one possible representation into a \"standard\", \"normal\", or canonical form (definition via Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_county(county_name):\n",
    "    return (\n",
    "        county_name\n",
    "        .lower()               # lower case\n",
    "        .replace(' ', '')      # remove spaces\n",
    "        .replace('&', 'and')   # replace &\n",
    "        .replace('.', '')      # remove dot\n",
    "        .replace('county', '') # remove county\n",
    "        .replace('parish', '') # remove parish\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop['clean_county'] = county_and_pop['County'].map(canonicalize_county)\n",
    "county_and_state['clean_county'] = county_and_state['County'].map(canonicalize_county)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the tables inline.  Here we use the builtin display function from iPython. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(county_and_pop)\n",
    "display(county_and_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas String Operations\n",
    "\n",
    "If this were a large dataframe it would have been more efficient to use pandas `str` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_county_series(count_name_series):\n",
    "    return (\n",
    "        count_name_series\n",
    "        .str.lower()               # lower case\n",
    "        .str.replace(' ', '')      # remove spaces\n",
    "        .str.replace('&', 'and')   # replace &\n",
    "        .str.replace('.', '')      # remove dot\n",
    "        .str.replace('county', '') # remove county\n",
    "        .str.replace('parish', '') # remove parish\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop['clean_county'] = canonicalize_county_series(county_and_pop['County'])\n",
    "county_and_state['clean_county'] = canonicalize_county_series(county_and_state['County'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(county_and_pop)\n",
    "display(county_and_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finishing the join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_and_pop.merge(county_and_state,\n",
    "                     left_on = 'clean_county', right_on = 'clean_county')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "# (Demo 2) Processing Data from a Text Log Using Basic Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('log.txt', 'r') as f:\n",
    "    log_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to extract the day, month, year, hour, minutes, seconds, and timezone. Looking at the data, we see that these items are not in a fixed position relative to the beginning of the string. \n",
    "\n",
    "That is, slicing by some fixed offset isn't going to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[0][20:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[1][20:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll need to use some more sophisticated thinking. Let's focus on only the first line of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = log_lines[0]\n",
    "first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with text it helps to grab a sample line.  However, you will want to check that your rule generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertinent = first.split(\"[\")[1].split(']')[0]\n",
    "pertinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day, month, rest = pertinent.split('/')\n",
    "print(\"day =\", day)\n",
    "print(\"month =\", month)\n",
    "print(\"rest =\", rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year, hour, minute, rest = rest.split(':')\n",
    "print(\"year =\", year)\n",
    "print(\"hour =\", hour)\n",
    "print(\"minute =\", minute)\n",
    "print(\"rest =\", rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds, time_zone = rest.split(' ')\n",
    "day, month, year, hour, minute, seconds, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting this into Dataframe Operations\n",
    "\n",
    "Suppose this was a column of text in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"text\": log_lines})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often helpful to extract the series to get better tab completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.str.split(\"[\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the command on multiple lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ts\n",
    "     .str.split(\"[\").str[1]\n",
    "     .str.split(\"]\").str[0]\n",
    "     .str.split(\"/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ts\n",
    "     .str.split(\"[\").str[1]\n",
    "     .str.split(\"]\").str[0]\n",
    "     .str.split(\"/\", expand=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (ts\n",
    "       .str.split(\"[\").str[1]\n",
    "       .str.split(\"]\").str[0]\n",
    "       .str.split(\"/\", expand=True)\n",
    "       .rename(columns={0:\"Day\", 1:\"Month\", 2:\"Rest\"})\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2 = df2[\"Rest\"]\n",
    "ts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = (\n",
    "    ts2.str.split(\" \").str[0]\n",
    "        .str.split(\":\",expand=True)\n",
    "        .rename(columns={0:\"Year\", 1:\"Hour\", 2: \"Minute\", 3:\"Second\"})\n",
    ")\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[['Day', 'Month']].join(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Taste of Regex \n",
    "\n",
    "A much more sophisticated but common approach is to extract the information we need using a regular expression. See today's lecture slidesfor more on regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'\\[(\\d+)/(\\w+)/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "day, month, year, hour, minute, second, time_zone = re.findall(pattern, first)[0]\n",
    "year, month, day, hour, minute, second, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can return the results as a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Day\", \"Month\", \"Year\", \"Hour\", \"Minute\", \"Second\", \"Time Zone\"]\n",
    "def log_entry_to_series(line):\n",
    "    return pd.Series(re.findall(pattern, line)[0], index = cols)\n",
    "\n",
    "log_entry_to_series(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using this function we can create a DataFrame of all the time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_info = pd.DataFrame(columns=cols)\n",
    "\n",
    "for line in log_lines:\n",
    "    log_info = log_info.append(log_entry_to_series(line), ignore_index = True)\n",
    "\n",
    "log_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas String Operations \n",
    "\n",
    "A more efficient way to do these transformations would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.str.extract(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ts.str.extract(pattern, expand=True)\n",
    "        .rename(columns={\n",
    "            0:\"Day\", 1:\"Month\", 2:\"Year\",\n",
    "            3:\"Hour\", 4:\"Minute\", 5:\"Second\",\n",
    "            6:\"TimeZone\"\n",
    "        })\n",
    "        .astype({\"Day\":int, \"Year\":int, \"Hour\":int, \"Second\":int})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "--- \n",
    "\n",
    "# Regular Expression From Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the regex below so that after code executes, day is “26”, month is “Jan”, and year is “2014”. \n",
    "\n",
    "<details>\n",
    "    <summary>Solution</summary>\n",
    "pattern = r\"\\[(\\d+)/(\\w+)/(\\d+):\"\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"fill me in\"\n",
    "matches = re.findall(pattern, log_lines[0])\n",
    "print(matches)\n",
    "# day, month, year = matches[0]\n",
    "#day, month, year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Example #1: Restaurant Data\n",
    "\n",
    "In this example, we will show how regexes can allow us to track quantitative data across categories defined by the appearance of various text fields.\n",
    "\n",
    "In this example we'll see how the presence of certain keywords can affect quantitative data, e.g. how do restaurant health scores vary as a function of the number of violations that mention \"vermin\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio = pd.read_csv('violations.csv.bz2', header=0, names=['id', 'date', 'desc'])\n",
    "desc = vio['desc']\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = desc.value_counts()\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmm...\n",
    "counts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use regular expressions to cut out the extra info in square braces.\n",
    "vio['clean_desc'] = (\n",
    "    vio['desc']\n",
    "             .str.replace('\\s*\\[[^\\]]*\\]$', '')\n",
    "             .str.strip()\n",
    "             .str.lower())\n",
    "print(\"Before:\", \"#\" + vio['desc'].iloc[-1] + \"#\")\n",
    "print(\"After:\", \"#\" + vio['clean_desc'].iloc[-1] + \"#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "# Extracting interesting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['clean_desc'].value_counts().head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use regular expressions to assign new features for the presence of various keywords\n",
    "with_features = (vio\n",
    " .assign(is_clean     = vio['clean_desc'].str.contains('clean|sanit'))\n",
    " .assign(is_high_risk = vio['clean_desc'].str.contains('high risk'))\n",
    " .assign(is_vermin    = vio['clean_desc'].str.contains('vermin'))\n",
    " .assign(is_surface   = vio['clean_desc'].str.contains('wall|ceiling|floor|surface'))\n",
    " .assign(is_human     = vio['clean_desc'].str.contains('hand|glove|hair|nail'))\n",
    " .assign(is_permit    = vio['clean_desc'].str.contains('permit|certif'))\n",
    ")\n",
    "with_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring aggregate features for each inspection (business id and date). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features = (with_features\n",
    " .groupby(['id', 'date'])\n",
    " .sum()\n",
    " .reset_index()\n",
    ")\n",
    "count_features.iloc[255:260, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features[count_features['is_vermin'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a new pandas feature called \"melt\" that we won't describe in any detail\n",
    "#the granularity of the resulting frame is a violation type in a given inspection\n",
    "broken_down_by_violation_type = pd.melt(count_features, id_vars=['id', 'date'],\n",
    "            var_name='feature', value_name='num_vios')\n",
    "broken_down_by_violation_type.sort_values([\"id\", \"date\"]).head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the scores\n",
    "ins = pd.read_csv('inspections.csv.bz2',\n",
    "                  header=0,\n",
    "                  usecols=[0, 1, 2],\n",
    "                  names=['id', 'score', 'date'])\n",
    "ins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join scores with the table broken down by violation type\n",
    "violation_type_and_scores = (\n",
    "    broken_down_by_violation_type\n",
    "    .merge(ins, left_on=['id', 'date'], right_on=['id', 'date'])\n",
    ")\n",
    "violation_type_and_scores.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='num_vios', y='score',\n",
    "               col='feature', col_wrap=2,\n",
    "               kind='box',\n",
    "               data=violation_type_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see, for example, that if a restaurant inspection involved 2 violation with the keyword \"vermin\", the average score for that inspection would be a little bit below 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Example 2: Police Data\n",
    "\n",
    "In this example, we will apply string processing to the process of data cleaning and exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data\n",
    "\n",
    "The city of Berkeley maintains an [Open Data Portal](https://data.cityofberkeley.info/) for citizens to access data about the city.  We will be examining [Call Data](https://data.cityofberkeley.info/Public-Safety/Berkeley-PD-Calls-for-Service/k2nh-s5h5).\n",
    "\n",
    "<img src=\"calls_desc.png\" width=800px />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.115040Z",
     "start_time": "2018-02-02T15:15:19.070620Z"
    }
   },
   "outputs": [],
   "source": [
    "import ds100_utils\n",
    "\n",
    "calls_url = 'https://data.cityofberkeley.info/api/views/k2nh-s5h5/rows.csv?accessType=DOWNLOAD'\n",
    "calls_file = ds100_utils.fetch_and_cache(calls_url, 'calls.csv')\n",
    "calls = pd.read_csv(calls_file, warn_bad_lines=True)\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records did we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.122345Z",
     "start_time": "2018-02-02T15:15:19.118071Z"
    }
   },
   "outputs": [],
   "source": [
    "len(calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does an example `Block_Location` value look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calls['Block_Location'].iloc[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Preliminary observations on the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "1. `EVENTDT` -- Contains the incorrect time\n",
    "1. `EVENTTM` -- Contains the time in 24 hour format (What timezone?)\n",
    "1. `CVDOW` -- Encodes the day of the week (see data documentation).\n",
    "1. `InDbDate` -- Appears to be correctly formatted and appears pretty consistent in time.\n",
    "1. **`Block_Location` -- a multi-line string that contains coordinates.**\n",
    "1. `BLKADDR` -- Appears to be the address in `Block Location`.\n",
    "1. `City` and `State` seem redundant given this is supposed to be the city of Berkeley dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting locations\n",
    "\n",
    "The block location contains geographic coordinates. Let's extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.230720Z",
     "start_time": "2018-02-02T15:15:19.225971Z"
    }
   },
   "outputs": [],
   "source": [
    "calls['Block_Location'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.268222Z",
     "start_time": "2018-02-02T15:15:19.233193Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_lat_lon = (\n",
    "    calls['Block_Location']\n",
    "    .str.extract(\"\\((\\d+\\.\\d+)\\, (-\\d+\\.\\d+)\\)\")\n",
    ")\n",
    "calls_lat_lon.columns = ['Lat', 'Lon']\n",
    "calls_lat_lon.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.280471Z",
     "start_time": "2018-02-02T15:15:19.271307Z"
    }
   },
   "outputs": [],
   "source": [
    "calls_lat_lon.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls[calls_lat_lon.isnull().any(axis=1)]['Block_Location'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join in the extracted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls['Lat'] = calls_lat_lon['Lat']\n",
    "calls['Lon'] = calls_lat_lon['Lon']\n",
    "calls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Location information\n",
    "\n",
    "Let's examine the geographic data (latitude and longitude).  Recall that we had some missing values.  Let's look at the behavior of these missing values according to crime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:34.130428Z",
     "start_time": "2018-02-02T15:15:32.851717Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_lat_lon = calls[calls[['Lat', 'Lon']].isnull().any(axis=1)]\n",
    "(missing_lat_lon['CVLEGEND'].value_counts() \n",
    " / calls['CVLEGEND'].value_counts()\n",
    ").sort_values(ascending=False).iplot(kind=\"barh\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a crime map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to install the folium package for this to work\n",
    "# to do this, uncomment the line below and run it, this might take \n",
    "# more than 5 minutes. It'll probably say \"solving environment: \" for\n",
    "# a long time.\n",
    "#!conda --debug install -y -c conda-forge folium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import folium.plugins\n",
    "\n",
    "SF_COORDINATES = (37.87, -122.28)\n",
    "sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13)\n",
    "locs = calls[['Lat', 'Lon']].astype('float').dropna().values\n",
    "heatmap = folium.plugins.HeatMap(locs.tolist(), radius=10)\n",
    "sf_map.add_child(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. Is campus really the safest place to be?\n",
    "1. Why are all the calls located on the street and at often at intersections?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium.plugins\n",
    "\n",
    "# Need to sample the data otherwise browser has issues.\n",
    "locations = calls[['Lat', 'Lon', \"OFFENSE\"]].sample(1000)\n",
    "\n",
    "cluster = folium.plugins.MarkerCluster()\n",
    "for _, r in locations.dropna().iterrows():\n",
    "    cluster.add_child(\n",
    "        folium.Marker([float(r[\"Lat\"]), float(r[\"Lon\"])], tooltip=r['OFFENSE']))\n",
    "    \n",
    "sf_map = folium.Map(location=SF_COORDINATES, zoom_start=13)\n",
    "sf_map.add_child(cluster)\n",
    "sf_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
