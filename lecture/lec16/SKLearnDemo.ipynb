{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Regression in SKLearn\n",
    "\n",
    "In this mini video lecture notebook we will introduce how to fit linear models using Scikit Learn.  Scikit is a library containing a wide number of classic machine learning algorithms and is to machine learning what pandas is to data in Python.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \n",
    "\n",
    "In this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support embedding YouTube Videos in Notebooks\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Walk-Through\n",
    "The following video walk-through presents this notebook.  I did not break it into sections but will try to do that in my next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"lFzRiinHSzU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For this notebook, we will focus on a synthetic dataset which is easy to visualize since it has two feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data = pd.read_csv(\"data/synth_data.csv.zip\")\n",
    "synth_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is simple enough that we can easily visualize it.  Take the data for a spin (literally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "data_scatter = go.Scatter3d(x=synth_data[\"X1\"], y=synth_data[\"X2\"], z=synth_data[\"Y\"], \n",
    "                            mode=\"markers\",\n",
    "                            marker=dict(size=2))\n",
    "fig.add_trace(data_scatter)\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Regression without SKLearn\n",
    "\n",
    "We can start using the normal equations:\n",
    "\n",
    "$$\n",
    " \\hat{\\theta} = \\left( \\mathbb{X}^T \\mathbb{X} \\right)^{-1} \\mathbb{X}^T \\mathbb{Y}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is not as numerically stable or efficient. We can compute $\\hat{\\theta}$ by direction using matrix inversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv, solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_by_inv(X, Y):\n",
    "    return inv(X.T @ X) @ X.T @ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient way to solve the normal equations is using the `solve` function to solve the linear systems:\n",
    "\n",
    "$$\n",
    "A \\theta = b\n",
    "$$\n",
    "\n",
    "where $A=\\mathbb{X}^T \\mathbb{X}$ and $b=\\mathbb{X}^T \\mathbb{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_by_solve(X, Y):\n",
    "    return solve(X.T @ X, X.T @ Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Synthetic Data\n",
    "\n",
    "Let's quickly test the above code on our synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = synth_data[[\"X1\", \"X2\"]].to_numpy()\n",
    "Y = synth_data[[\"Y\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = least_squares_by_inv(X,Y)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = least_squares_by_solve(X,Y)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both agree! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the Linear Model\n",
    "\n",
    "For the synthetic dataset we can visualize the model in three dimensions.  To do this we will use the following plotting function that at evenly space grid points in for `X0` and `X1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_plane(f, X, grid_points = 30):\n",
    "    u = np.linspace(X[:,0].min(),X[:,0].max(), grid_points)\n",
    "    v = np.linspace(X[:,1].min(),X[:,1].max(), grid_points)\n",
    "    xu, xv = np.meshgrid(u,v)\n",
    "    X = np.vstack((xu.flatten(),xv.flatten())).transpose()\n",
    "    z = f(X)\n",
    "    return go.Surface(x=xu, y=xv, z=z.reshape(xu.shape),opacity=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data and the plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "fig.add_trace(plot_plane(lambda x: x.dot(least_squares_by_inv(X,Y)), X))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something is Wrong!\n",
    "\n",
    "The above plane doesn't seem that close to the data.  What did we do wrong?  Let's add a cyan colored sphere at the origin (0,0,0) in the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "fig.add_trace(plot_plane(lambda x: x.dot(least_squares_by_inv(X,Y)), X))\n",
    "fig.add_trace(go.Scatter3d(x=[0], y=[0], z=[0], marker=dict(color=\"cyan\")))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plane is passing through the origin.  Actually, any parameterization of the current model must go through the origin.  Why is that?  Let's look at the current equation for our model:\n",
    "\n",
    "\\begin{align}\n",
    "f_\\theta(x_1, x_2) = \\theta_1 x_1 + \\theta_2 x_2\n",
    "\\end{align}\n",
    "\n",
    "What is the value of this function when $x_1=0$ and $x_2=0$?  Does it depend $\\theta$?  What are we missing?\n",
    "\n",
    "<br/><br/><br/>\n",
    "\n",
    "<details>\n",
    "    <summary>Answer </summary>\n",
    "\n",
    "We forgot to add a constant term (a 1s column) to our $\\mathbb{X}$ covariate matrix.  \n",
    "</details>\n",
    "\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a 1s Column\n",
    "\n",
    "We would like the model to be:\n",
    "\n",
    "\\begin{align}\n",
    "f_\\theta(x_1, x_2) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2\n",
    "\\end{align}\n",
    "\n",
    "We can do this by adding an $x_0$ (an extra column) which always takes the value 1.\n",
    "\n",
    "\\begin{align}\n",
    "f_\\theta(x_0, x_1, x_2) = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2\n",
    "\\end{align}\n",
    "\n",
    "By doing this we can keep the general form of the **linear equation** (no need for special cases):\n",
    "\n",
    "\\begin{align}\n",
    "f_\\theta(x) = \\sum_{i=1}^p \\theta_i x_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function adds a 1s column to our matrix.  We are horizontally stacking a 1 column vector with our covariate matrix to make a new covariate matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones_column(X):\n",
    "    return np.hstack([np.ones((X.shape[0],1)), X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ones_column(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolving for the optimal $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_squares_by_solve(add_ones_column(X), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the model we need to be a bit more careful. My plotting code assumes that the $X$ matrix is $n$ by $2$ dimensions.  So we will make a special model function that transforms $X$ by adding an extra dimension and then renders the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = least_squares_by_solve(add_ones_column(X),Y)\n",
    "def model_append_ones(X):\n",
    "    return add_ones_column(X) @ theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking our new model for a spin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "fig.add_trace(plot_plane(model_append_ones, X))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success!!!** That seems to fit the data nicely!  That wasn't too hard but let's try doing using sklearn instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SKLearn\n",
    "\n",
    "Scikit Learn, or as the cool kids call it sklearn (pronounced s-k-learn), is an large package of useful machine learning algorithms. For this lecture, we will use the `LinearRegression` model in the [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module.  The fact that there is an entire module with many different models within the `linear_model` module might suggest that we have a lot to cover still (we do!).  \n",
    "\n",
    "**What you should know about `sklearn` models:**\n",
    "\n",
    "1. Models are created by first building an instance of the model:\n",
    "```python\n",
    "model = SuperCoolModelType(args)\n",
    "```\n",
    "1. You then fit the model by calling the **fit** function passing in data:\n",
    "```python\n",
    "model.fit(df[['X1' 'X1']], df[['Y']])\n",
    "```\n",
    "1. You then can make predictions by calling **predict**:\n",
    "```python\n",
    "model.predict(df2[['X1' 'X1']])\n",
    "```\n",
    "\n",
    "The neat part about sklearn is most models behave like this.  So if you want to try a cool new model you just change the class of mode you are using. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LinearRegression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `LinearRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model by passing it the $X$ and $Y$ data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(synth_data[[\"X1\", \"X2\"]], synth_data[[\"Y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some predictions and even save them back to the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_data['Y_hat'] = model.predict(synth_data[[\"X1\", \"X2\"]])\n",
    "synth_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model as before.  Notice I just pass the `model.predict` **function** to our visualization code which will invoke `model.predict` on a regular grid of points to make the surface plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "fig.add_trace(plot_plane(model.predict, X))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Model \"Hyper-Parameters\"\n",
    "\n",
    "Notice that when we created an instance of the model we set `fit_intercept=False`.  This is a **hyper-parameter** of the model.  What is a **hyper-parameter** these are the **parameters** that we do not optimize but instead choose as part of the modeling process.  In this case the `fit_intercept` hyper-parameter adds the additional $\\theta_0$ term to our model without adding a 1s column to the data.  Internally, it probably adds a 1s column to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we set the `fit_intercept=False`?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_intercept = LinearRegression(fit_intercept=False)\n",
    "model_no_intercept.fit(synth_data[[\"X1\", \"X2\"]], synth_data[[\"Y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "fig.add_trace(plot_plane(model_no_intercept.predict, X))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying an Advanced Model\n",
    "\n",
    "In an earlier lecture we alluded to Kernel Regression as a more advanced technique.  To demonstrate the simplicity and power of sklearn lets try using a Kernel regression model instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `KernelRidge` model and setting the kernel function type to be RBF which stands for radial basis functions and nothing else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model = KernelRidge(kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the RBF KernelRidge regression model to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model.fit(synth_data[[\"X1\", \"X2\"]], synth_data[[\"Y\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(data_scatter)\n",
    "fig.add_trace(plot_plane(super_model.predict, X))\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), \n",
    "                  height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we basically did exactly the same thing as before but with a different model class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
