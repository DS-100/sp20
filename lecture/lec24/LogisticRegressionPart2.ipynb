{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In the previous notebook we discussed why least-squares linear regression is not well suited to modeling classification tasks.  In this notebook we introduce logistic regression for modeling classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"TIXc813Ql-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "As with other notebooks we will use the same set of standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "We will continue to use the [Wisconsin Breast Cancer Dataset](http://scikit-learn.org/stable/datasets/index.html#breast-cancer-wisconsin-diagnostic-database). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.590723Z",
     "start_time": "2018-04-02T16:07:17.472304Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "data = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "# Target data_dict['target'] = 0 is malignant 1 is benign\n",
    "data['malignant'] = (data_dict['target'] == 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data Train-Test Split\n",
    "\n",
    "Always split your data into training and test groups.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.924717Z",
     "start_time": "2018-04-02T16:07:17.899036Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_tr, data_te = train_test_split(data, test_size=0.10, random_state=42)\n",
    "print(\"Training Data Size: \", len(data_tr))\n",
    "print(\"Test Data Size: \", len(data_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the `X` and `Y` matrices for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_tr[['mean radius']].to_numpy()\n",
    "Y = data_tr['malignant'].astype(float).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = go.Scatter(x=X.flatten(), y = Y,\n",
    "                    mode=\"markers\", \n",
    "                    marker=dict(opacity=0.5))\n",
    "layout = dict(xaxis=dict(title=\"Mean Radius\"),yaxis=dict(title=\"Malignant\"))\n",
    "go.Figure(data=[points], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lecture we implemented a `jitter` function to jitter the data to make it easier to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(data, amt=0.1):\n",
    "    return data + amt * np.random.rand(len(data)) - amt/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.773256Z",
     "start_time": "2018-04-02T16:07:17.757563Z"
    }
   },
   "outputs": [],
   "source": [
    "points = go.Scatter(x=X.flatten(), y = jitter(Y), \n",
    "                    mode=\"markers\", \n",
    "                    marker=dict(opacity=0.5))\n",
    "layout = dict(xaxis=dict(title=\"Mean Radius\"),yaxis=dict(title=\"Malignant\"))\n",
    "go.Figure(data=[points], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a little missleading so let's try a different visualization for this lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.773256Z",
     "start_time": "2018-04-02T16:07:17.757563Z"
    }
   },
   "outputs": [],
   "source": [
    "points = go.Scatter(name=\"Training Data\", x=X.flatten(), y = Y, \n",
    "                    mode=\"markers\", \n",
    "                    marker=dict(symbol=\"line-ns\", size=5, line=dict(width=1, color=\"darkblue\"))\n",
    "                   )\n",
    "layout = dict(xaxis=dict(title=\"Mean Radius\"),yaxis=dict(title=\"Malignant\"))\n",
    "go.Figure(data=[points], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "## Least Squares Linear Regresion Model\n",
    "\n",
    "In the previous notebook we fit a linear model to this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:20:28.524852Z",
     "start_time": "2018-04-03T08:20:28.397599Z"
    }
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plt = np.expand_dims(np.linspace(X.min(), X.max(), 100),1)\n",
    "model_line = go.Scatter(name=\"Least Squares\",\n",
    "    x=X_plt.flatten(), y=lin_reg.predict(X_plt), \n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "go.Figure([points, model_line], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous notebook, we noted several problems with the use of least squares linear regression for classification.\n",
    "\n",
    "1. Predictions were not 0 or 1 but instead continuous\n",
    "2. Treaing the predicitons as a probability also doesn't work since they are not between 0 and 1.\n",
    "3. The least squares loss could produce an arbitrarily bad model for extreme data points.\n",
    "\n",
    "In this notebook we will address these issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "## Empirical Probability \n",
    "\n",
    "A natural starting place for modeling a categorical variable (e.g., whether a tumor is benign or malignant) is to model the probability of whether it is benign or malignant.  We could start with the simplest model predict a constant probability: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"WJnz-ELJ5e4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_malignant = np.mean(Y)\n",
    "print(\"Proability of being malignant:\", pr_malignant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_pr_model(X):\n",
    "    return pr_malignant * np.ones(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus for any radius we would just return the same probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_prob_line = go.Scatter(name=\"Constant Probability\",\n",
    "    x=X_plt.flatten(), y=constant_pr_model(X),\n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "go.Figure([points, constant_prob_line], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above constant model doesn't depend on the **mean radius**.  We could improve upon this model by computing a constant for different bins of the mean radius.  The following block of code divides the x-axis (mean radius) into 10 regions and computes the proportion of malignant tumors in each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_splits = np.linspace(6.5, 28.5, 15)\n",
    "pr_mal_split = np.zeros(len(X_splits))\n",
    "for i in range(0, len(X_splits)-1):\n",
    "    pr_mal_split[i] = np.mean(Y[((X > X_splits[i]) & (X <= X_splits[i+1])).flatten()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([points], layout=layout)\n",
    "for i in range(len(X_splits)):\n",
    "    fig.add_shape(type=\"line\", x0=X_splits[i], x1=X_splits[i], y0=-.2, y1=1.2, line=dict(color=\"LightSeaGreen\",\n",
    "                width=1,\n",
    "                dash=\"dashdot\",\n",
    "            ))\n",
    "# fig.add_trace(go.Scatter(name = \"Prop. Malignant\", x=X_middle, y=pr_mal_split))\n",
    "splits_plot = go.Scatter(name = \"Prop. Malignant Split\", \n",
    "                         x=np.vstack([X_splits[:-1], X_splits[1:]]).T.flatten(), \n",
    "                         y=np.vstack([pr_mal_split, pr_mal_split]).T.flatten(),\n",
    "                         line=dict(color=\"orange\", width=3))\n",
    "fig.add_trace(splits_plot)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually a pretty reasonable model but if we had higher dimensional features, dividing the space into bins would get exponentially more expensive.  In addition, many (most) of the bins would have no points so it would be difficult to estimate the proportions in that bin.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest Neighbors (Bonus)\n",
    "\n",
    "Rather than dividing the space into bins we could instead consider the proportion of tumors that are malignant and have \"similar\" features (e.g., mean radius) to the tumor for which we would like to make a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "import heapq\n",
    "\n",
    "def knearest_neighbors(x, X_tr, Y_tr, k=5):\n",
    "    # Compute the distance\n",
    "    dist = norm(x - X_tr, axis=1)\n",
    "    # Predict the average Y value of the k closest data points to x\n",
    "    return np.mean(Y_tr[heapq.nsmallest(k, range(len(dist)), dist.__getitem__)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.add_trace(\n",
    "    go.Scatter(name = \"K Nearest Neighbors\", \n",
    "               x=X_plt.flatten(), \n",
    "               y=[knearest_neighbors(x, X, Y, 91) for x in X_plt.flatten()],\n",
    "               line=dict(color=\"red\", width=3))\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "## The Logistic Regression Model\n",
    "\n",
    "Logistic regression is probably one of the most widely used basic models for classification and is a simple extension of linear models to the classification problem.  In the remainder of this notebook we walk through the logistic function and how to fit logistic regression models using scikit-learn.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"U4TeibU_Q60\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model predicts the **probability** that the **binary** $Y$ variable (e.g., is the tumor malignant) will take the value 1 given the features $x$ (e.g., the mean radius of the tumor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{P}_\\theta\\left(Y=1 \\,|\\, x\\right) = f_\\theta(x) = \\frac{1}{1 + \\exp\\left(-\\sum_{k=0}^d \\theta_k x_k\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with our linear model, the above function is **parameterized** by the vector $\\theta$ and is a **function of** our basic linear model $\\sum_{k=0}^d \\theta_k x_k$.  However, this is no longer a **linear model** but instead often called a **generalized linear model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Logistic Activation Function\n",
    "\n",
    "The logistic regression model derives its name from the **logistic function**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{logistic}(t) = \\sigma(t) = \\frac{1}{1+\\exp(-t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function is also often called the sigmoid function and denoted $\\sigma(t)$.  Sadly, the greek letter $\\sigma$ is widely used to mean a lot of things (e.g., standard deviation, logistic function, permutations) and so you will have to guess the meaning from context.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the logistic regression model in the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{P}\\left(Y=1 \\,|\\, x\\right) = f_\\theta(x) =\\sigma\\left(\\sum_{k=0}^d \\theta_k x_k\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the logistic function maps the output of the linear model (which could be any real number) to the interval between 0 and 1  and is commonly used when modeling probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the logistic to see it's characteristic (s-shape, sigmoid-shape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:20:28.586913Z",
     "start_time": "2018-04-03T08:20:28.583811Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(t):\n",
    "    return 1. / (1. + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-03T08:20:28.597034Z",
     "start_time": "2018-04-03T08:20:28.588719Z"
    }
   },
   "outputs": [],
   "source": [
    "t = np.linspace(-5,5,50)\n",
    "sigmoid_line = go.Scatter(name=\"Logistic Function\",\n",
    "    x=t, y=logistic(t), mode=\"lines\")\n",
    "go.Figure([sigmoid_line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Dimensional Logistic Regression Model\n",
    "\n",
    "To get an intuition for the behavior of the parameters in the logistic regression model let's consider a simple one dimensional logistic regression model of the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{P}\\left(Y=1 \\,|\\, x\\right) =\\sigma\\left(\\theta_0 + \\theta_1 x\\right) = \\frac{1}{1+\\exp\\left(-\\theta_0 - \\theta_1 x\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two plots allow us to vary $\\theta_0$ and $\\theta_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for theta1 in [-1,1, 5]:\n",
    "    for theta0 in [-2, 0, 2]:\n",
    "        fig.add_trace(go.Scatter(name=f\"{theta0} + {theta1} x\", x=t, y=logistic(theta0 + theta1*t)))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Loss\n",
    "\n",
    "Because the logistic regression model predicts a probability we typically use the **negative log-likelihood** as the loss function.  This is also called the **cross entropy** loss and is written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\theta) = -\\sum_{i=1}^n y_i \\log\\left(f_\\theta(x_i)\\right) + (1-y_i) \\log\\left(1-f_\\theta(x_i)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the squared loss, there is no closed form solution to this loss function and so iterative methods like gradient descent are typically used to minimize the loss function with respect to the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Minimizing the Loss Using SGD and Pytorch\n",
    "\n",
    "In general, you should use scikit-learn or other frameworks that have specialized implementations for logistic regression model fitting.  This is because you can often use more advanced iterative algorithms to fit the logistic regression model.  However, to demonstrate how these iterative methods work, we can implement logistic regression using PyTorch and solve for the optimal parameters using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"thEZGXIfJqc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Logistic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LogisticModel(nn.Module):\n",
    "    def __init__(self, w=None):\n",
    "        super().__init__()\n",
    "        # Creating a nn.Parameter object allows torch to track parameters for us\n",
    "        if w is not None: \n",
    "            self.w = nn.Parameter(torch.from_numpy(w))\n",
    "        else: \n",
    "            self.w = nn.Parameter(torch.zeros(2,1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w = self.w\n",
    "        return 1/(1 + torch.exp(-(w[0] + w[1] * x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_lr_model = LogisticModel(np.array([0.,1.]))\n",
    "torch_lr_model.forward(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Cross Entropy Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\theta) = -\\sum_{i=1}^n y_i \\log\\left(f_\\theta(x_i)\\right) + (1-y_i) \\log\\left(1-f_\\theta(x_i)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_cross_entropy_loss(P_hat, Y):\n",
    "    return -torch.sum(Y * torch.log(P_hat) + (1-Y) * torch.log(1 - P_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting Data to PyTorch Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "tensor_data = TensorDataset(torch.from_numpy(X.flatten()), \n",
    "                            torch.from_numpy(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "def adam_sgd(model, loss_fn, dataset, lr=.1, nepochs=100, batch_size=10):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    opt = Adam(model.parameters(), lr=lr)\n",
    "    for i in range(nepochs):\n",
    "        for (x, y) in loader:\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the Optimizer on The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_lr_model = LogisticModel(np.array([0.,1.]))\n",
    "adam_sgd(torch_lr_model, torch_cross_entropy_loss, tensor_data, lr=0.1)\n",
    "torch_lr_model.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch_p_hats = torch_lr_model.forward(torch.from_numpy(X_plt.flatten())).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([points], layout=layout)\n",
    "pytorch_lr_line = go.Scatter(name = \"Pytorch Logistic Regression\", \n",
    "                         x=X_plt.flatten(), y=torch_p_hats,\n",
    "                         line=dict(color=\"cyan\", width=3))\n",
    "fig.add_trace(pytorch_lr_line)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Regularization\n",
    "\n",
    "Just as with linear regression, L1 and L2 regularization can and are often applied to the logistic regression loss.  Later we will see why L2 regularization is almost always used with logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Using Scikit-Learn for Logistic Regression\n",
    "\n",
    "Logistic regression models in scikit-learn follow the same API as all other models and therefore you already know how to use them.  First we import the logistic regression model.  Notice that we are importing it from the linear models module.  Logistic regression isn't technically a linear model but instead a generalized linear model (a linear model with a non-linear transformation).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"tQL4rCD6PFU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(solver=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "When making a prediction the `predict` function returns the predicted (most likely class). while the `predict_proba` returns the predicted probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.predict(np.array([[20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.predict_proba(np.array([[12]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the Logistic Regression Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([points, pytorch_lr_line], layout=layout)\n",
    "p_hats = lr_model.predict_proba(X_plt)\n",
    "lr_line = go.Scatter(name = \"Sklearn Logistic Regression\", \n",
    "                         x=X_plt.flatten(), y=p_hats[:,1],\n",
    "                         line=dict(color=\"orange\", width=3))\n",
    "fig.add_trace(lr_line)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the curves are slightly different.  There are a few reasons.  First, the Pytorch optimization above is not as well tuned as the LBFGS solver that sklearn is using.  Second, and perhaps more important sklearn is using a small amount of L2 regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(lr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separable Data and the Need For Regularization\n",
    "\n",
    "Supposed we had the following toy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_X = np.array([[-1.0, -0.2, 0.2, 1.0]])\n",
    "toy_Y = np.array([0.0, 0.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_points = go.Scatter(name=\"Toy Data\", x=toy_X.flatten(), y=toy_Y, mode='markers',\n",
    "                        marker=dict(size=10))\n",
    "go.Figure([toy_points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simplified logistic regression model of the form:\n",
    "\n",
    "$$\n",
    "\\hat{P}\\left(Y=1 \\,|\\, x\\right) =\\sigma\\left(\\theta_1 x\\right) = \\frac{1}{1+\\exp\\left(- \\theta_1 x\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_model(theta1, x):\n",
    "    return 1/(1 + np.exp(-theta1 * x))\n",
    "\n",
    "def cross_entropy_loss(theta1, x, y):\n",
    "    # Here we use 1 - sigma(t) = sigma(-t) to improve numerical stability\n",
    "    return - np.sum(y * np.log(toy_model(theta1, x)) + (1-y) * np.log(toy_model(theta1, -x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try a range of values for $\\theta_1$ we notice that we can keep making $\\theta_1$ larger and further reducing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1s = np.array([1, 2, 5, 10, 15, 20, 25, 50, 100])\n",
    "\n",
    "loss_curve = go.Scatter(x=theta1s, y=[cross_entropy_loss(t, toy_X, toy_Y) for t in theta1s])\n",
    "go.Figure([loss_curve], layout=dict(yaxis=dict(title=\"Cross Entropy Loss\", type=\"log\"), \n",
    "                                    xaxis=dict(title=r\"$$\\theta_1$$\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to the probabilities moving closer to 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_xplt = np.linspace(-1.2,1.2,100)\n",
    "fig = go.Figure([toy_points])\n",
    "for t in theta1s:\n",
    "    fig.add_trace(go.Scatter(name=f\"theta1={t}\", x=toy_xplt, y=toy_model(t, toy_xplt)))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the limit the sigmoid curve will become a step function.  If we introduce a neglible amount of L2 regularization we can avoid this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_with_reg(theta1, x, y):\n",
    "    # Here we use 1 - sigma(t) = sigma(-t) to improve numerical stability\n",
    "    return - np.sum(y * np.log(toy_model(theta1, x)) + (1-y) * np.log(toy_model(theta1, -x))) + 1.0e-5 * theta1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1s = np.array([1, 2, 5, 10, 15, 20, 25, 50, 100])\n",
    "\n",
    "loss_curve = go.Scatter(x=theta1s, y=[cross_entropy_loss_with_reg(t, toy_X, toy_Y) for t in theta1s])\n",
    "go.Figure([loss_curve], layout=dict(yaxis=dict(title=\"Cross Entropy Loss\", type=\"log\"), \n",
    "                                    xaxis=dict(title=r\"$$\\theta_1$$\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Interpreting the Probabilities\n",
    "\n",
    "In the last part of this notebook we walk through how we use the predicted probabilities to make a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"f6hLrFChkXU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decision Rule\n",
    "\n",
    "How do we use the probability to decide whether to classify a tumor as benign or malignant?  The sklearn logistic regression model `predict` function implements a basic decision rule:\n",
    "\n",
    "\n",
    "**Predict 1 iff $\\hat{P}\\left(Y=1 \\,|\\, x\\right) > 0.5$ otherwise predict 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(lr_model.predict(X) == np.where(lr_model.predict_proba(X)[:,1] > 0.5, 1.0, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could generalize this decision rule:\n",
    "\n",
    "**Predict 1 iff $\\hat{P}\\left(Y=1 \\,|\\, x\\right) > \\tau$ otherwise predict 0.**\n",
    "\n",
    "for any choice of $\\tau$.  Is $\\tau = 0.5$ the best threshold? It depends on our goals.  Suppose we wanted to maximize accuracy.  The **accuracy** of our classifier is defined as the fraction of correct predictions.  We can compute the accuracy for different thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_predict(model, X, threshold): \n",
    "    return np.where(lr_model.predict_proba(X)[:,1] > threshold, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(threshold, X, Y):\n",
    "    return np.mean(threshold_predict(lr_model, X, threshold) == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "accs = [accuracy(t, X, Y) for t in thresholds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=thresholds, y=accs)\n",
    "fig.update_xaxes(title=\"threshold\")\n",
    "fig.update_yaxes(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we should use cross validation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scorer(threshold, metric):\n",
    "    return lambda model, x, y: metric(y, threshold_predict(model, x, threshold)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "cv_accs = [\n",
    "    np.mean(cross_val_score(lr_model, X, Y, \n",
    "                            scoring=make_scorer(t, metrics.accuracy_score), \n",
    "                            cv=5))\n",
    "    for t in thresholds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=thresholds, y=cv_accs)\n",
    "fig.update_xaxes(title=\"threshold\")\n",
    "fig.update_yaxes(title=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the threshold with the highest accuracy is not necessarily at 0.5.  This can occur for many reasons but it is likely due here to class imbalance.  There are fewer malginant tumors and so we want to be more confident before classifying a tumor as malignant.\n",
    "\n",
    "The threshold should typically be tuned using cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "A convenient way to visualize the accuracy of a classification model is to look at the confusion matrix.  The confusion matrix compares what the model predicts with the actual counts in each class. The types of error are:\n",
    "\n",
    "1. **False-Positives**: When the actual class is 0 (false) but the algorithm predicts 1 (true).\n",
    "1. **False-Negatives**: When the actual class is 1 (true) but the algorithm predicts 0 (false).\n",
    "\n",
    "Ideally, we would like to minimize the sources of error but we may also want to manage the balance between these types of error.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a function to compute the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(Y, lr_model.predict(X))\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_annotated_heatmap(z=mat,\n",
    "                                  x=[\"False\", \"True\"], y=[\"False\", \"True\"], \n",
    "                                  showscale=True)\n",
    "fig.update_layout(font=dict(size=18))\n",
    "# Add Labels\n",
    "fig.add_annotation(x=0,y=0, text=\"True Negative\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"black\",size=24))\n",
    "fig.add_annotation(x=1,y=0, text=\"False Positive\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"white\",size=24))\n",
    "fig.add_annotation(x=0,y=1, text=\"False Negative\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"white\",size=24))\n",
    "fig.add_annotation(x=1,y=1, text=\"True Positive\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"white\",size=24))\n",
    "\n",
    "fig.update_xaxes(title=\"Predicted\")\n",
    "fig.update_yaxes(title=\"Actual\", autorange=\"reversed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Precision vs Recall\n",
    "\n",
    "In many settings, there might be a much higher cost to missing positive cases.  For example, if we were building a tumor classifier we would want to make sure that we don't miss any malignant tumors.  We might be prefer to classify benign tumors as malignant since further study could be conducted by pathologist to verify all malignant tumors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"gZmOmgQns3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these settings, we might want to adjust the **precision** or **recall**.  \n",
    "\n",
    "The following wikipedia illustration depicts the precision recall tradeoff.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precision vs Reca\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "The precision of a model is defined as:\n",
    "\n",
    "$$\n",
    "\\textbf{Precision} = \\frac{\\textbf{True Positives}}{\\textbf{True Positives} + \\textbf{False Positives}} = \\frac{\\textbf{True Positives}}{\\textbf{Predicted True}}\n",
    "$$\n",
    "\n",
    "and captures the accuracy of the model when it is positive. Higher precision models are often more likely to predict that true things are negative (higher false negative rate).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "The recall of a model is defined as:\n",
    "\n",
    "$$\n",
    "\\textbf{Recall} = \\frac{\\textbf{True Positives}}{\\textbf{True Positives} + \\textbf{False Negatives}} = \\frac{\\textbf{True Positives}}{\\textbf{Actually True}}\n",
    "$$\n",
    "\n",
    "and captures the ability of the model to predict true on all the true examples.  Higher recall runs the risk of predicting true on false examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision vs Recall\n",
    "\n",
    "A common analysis is to compare the precision and recall at different thresholds.  We can implement functions to compute the precision and recall at different thresholds and then consider all the thresholds given by our predictions and plot the precision versus the recall at each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_threshold(prob, threshold):\n",
    "    return np.where(prob >= threshold, 1., 0.)\n",
    "\n",
    "def precision_at_threshold(Y, prob, threshold):\n",
    "    Y_hat = predict_at_threshold(prob, threshold)\n",
    "    return np.sum((Y_hat == 1) & (Y == 1)) / np.sum(Y_hat)\n",
    "\n",
    "def recall_at_threshold(Y, prob, threshold):\n",
    "    Y_hat = predict_at_threshold(prob, threshold)\n",
    "    return np.sum((Y_hat == 1) & (Y == 1)) / np.sum(Y)\n",
    "\n",
    "def precision_recall_curve(Y, prob):\n",
    "    unique_thresh = np.unique(prob)\n",
    "    precision = [precision_at_threshold(Y, prob, t) for t in unique_thresh]\n",
    "    recall = [recall_at_threshold(Y, prob, t) for t in unique_thresh]\n",
    "    return precision, recall, unique_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(Y,  lr_model.predict_proba(X)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=recall, y = precision, hover_name=threshold)\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, there is an scikit-learn function to compute this tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold = metrics.precision_recall_curve(Y, lr_model.predict_proba(X)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision and recall are computed for all possible probability thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=recall[:-1], y = precision[:-1], hover_name=threshold)\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(name=\"Precision\", x=threshold, y=precision[:-1]))\n",
    "fig.add_trace(go.Scatter(name=\"Recall\", x=threshold, y=recall[:-1]))\n",
    "fig.update_xaxes(title=\"Threshold\")\n",
    "fig.update_yaxes(title=\"Proportion\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to ensure that 95% of the malignant tumors are classified as malignant we would want to select the following threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal95_ind = np.argmin(recall >= 0.95)-1\n",
    "mal95_thresh = threshold[mal95_ind]\n",
    "mal95_precision = precision[mal95_ind]\n",
    "mal95_recall = recall[mal95_ind]\n",
    "\n",
    "print(\"Threshold:\", mal95_thresh)\n",
    "print(\"Precision:\", mal95_precision)\n",
    "print(\"Recall:\", mal95_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we would actually want a pretty low threshold to ensure that we don't miss any malignant tumors.  We would then find that roughly 41% (1-precision) of the tumors we classify as malignant would actually be benign.  With this threshold what fraction of tumors would need to be processed in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proporition of samples below threshold:\", \n",
    "      np.mean(lr_model.predict_proba(X)[:,1] < mal95_thresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That  means that the pathologist would have to verify about 61% of the samples.  Using this model, we could reduce the workload in pathology by 39%.  However, we would falsely diagnose 5% of the tumors as benign when they are actually malignant and in practice that would be unacceptable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Model (Bonus)\n",
    "\n",
    "We could try to impove this model by using additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_full = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "lr_model_full.fit(data_tr.drop(columns='malignant'), data_tr['malignant'].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the precision-recall curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prb_mal = lr_model_full.predict_proba(data_tr.drop(columns='malignant'))[:,1]\n",
    "precision, recall, threshold = metrics.precision_recall_curve(data_tr['malignant'], prb_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=recall[:-1], y = precision[:-1], hover_name=threshold)\n",
    "fig.update_xaxes(title=\"Recall\")\n",
    "fig.update_yaxes(title=\"Precision\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the precision-recall curve is much further to the right.  This is a much better model.\n",
    "\n",
    "What threshold would we need to get **99% coverage**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mal95_ind = np.argmin(recall >= 0.99)-1\n",
    "mal95_thresh = threshold[mal95_ind]\n",
    "mal95_precision = precision[mal95_ind]\n",
    "mal95_recall = recall[mal95_ind]\n",
    "\n",
    "print(\"Threshold:\", mal95_thresh)\n",
    "print(\"Precision:\", mal95_precision)\n",
    "print(\"Recall:\", mal95_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the test data\n",
    "\n",
    "Now that we have finished our modeling process we can see how our model performs on the test data.  This would give us a better understanding of how it might perform on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prb_mal = lr_model_full.predict_proba(data_te.drop(columns='malignant'))[:,1]\n",
    "Y_hat = prb_mal >= mal95_thresh\n",
    "mat = confusion_matrix(data_te['malignant'], Y_hat)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_annotated_heatmap(z=mat,\n",
    "                                  x=[\"False\", \"True\"], y=[\"False\", \"True\"], \n",
    "                                  showscale=True)\n",
    "fig.update_layout(font=dict(size=18))\n",
    "# Add Labels\n",
    "fig.add_annotation(x=0,y=0, text=\"True Negative\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"black\",size=24))\n",
    "fig.add_annotation(x=1,y=0, text=\"False Positive\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"white\",size=24))\n",
    "fig.add_annotation(x=0,y=1, text=\"False Negative\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"white\",size=24))\n",
    "fig.add_annotation(x=1,y=1, text=\"True Positive\", \n",
    "                   yshift=40, showarrow=False, font=dict(color=\"white\",size=24))\n",
    "\n",
    "fig.update_xaxes(title=\"Predicted\")\n",
    "fig.update_yaxes(title=\"Actual\", autorange=\"reversed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
