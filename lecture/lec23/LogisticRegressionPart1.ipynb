{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Updated)\n",
    "\n",
    "This notebook accompanies the lecture on Logistic Regression and was updated to incorporate the new video-notebook format. If you have not already watched the accompanying lecture, you should do that first. \n",
    "\n",
    "In this notebook we walk through the (miss)application of least-squares regression to a binary classification task.  In the process, we will show why a different model and loss is needed.  We will then demonstrate how to use the scikit-learn logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "As with other notebooks we will use the same set of standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True, sharing=False, theme='ggplot');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "For this lecture, we will use the Wisconsin Breast Cancer Dataset which we can obtain from [scikit learn](http://scikit-learn.org/stable/datasets/index.html#breast-cancer-wisconsin-diagnostic-database). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"ComingSoon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of measurements from tumor biopsies for 569 patients as well as whether the tumor was malignant or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.590723Z",
     "start_time": "2018-04-02T16:07:17.472304Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "data = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Task\n",
    "\n",
    "The prediction task for this data is to predict whether a tumor is benign or malignant (a binary decision) given characteristics of that tumor. As a classic machine learning dataset, the prediction task is captured by the column named `\"target\"`.  To put the data back in it's original context we will create a new column called `\"malignant\"` which will be 1 if the tumor is malignant and 0 if it is benign (reversing the definition of target).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target data_dict['target'] = 0 is malignant 1 is benign\n",
    "data['malignant'] = (data_dict['target'] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features\n",
    "\n",
    "What features might be a good indication of whether a tumor is benign or malignant? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.599437Z",
     "start_time": "2018-04-02T16:07:17.593220Z"
    }
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a good starting point is the size of the tumor.  Larger tumors are probably more likely to be malignant.  In the following, we plot whether the tumor was malignant (1 or 0) against the `\"mean radius\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.755067Z",
     "start_time": "2018-04-02T16:07:17.601012Z"
    }
   },
   "outputs": [],
   "source": [
    "points = go.Scatter(x=data['mean radius'], y = 1.*data['malignant'], mode=\"markers\")\n",
    "layout = dict(xaxis=dict(title=\"Mean Radius\"),yaxis=dict(title=\"Malignant\"))\n",
    "go.Figure(data=[points], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a clear example of over-plotting.  We can improve the above plot by jittering the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(data, amt=0.1):\n",
    "    return data + amt * np.random.rand(len(data)) - amt/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.773256Z",
     "start_time": "2018-04-02T16:07:17.757563Z"
    }
   },
   "outputs": [],
   "source": [
    "points = go.Scatter(x=data['mean radius'], y = jitter(data['malignant']), \n",
    "                    mode=\"markers\", \n",
    "                    marker=dict(opacity=0.5))\n",
    "go.Figure(data=[points], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a better way to visualize the data is using stacked histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.896849Z",
     "start_time": "2018-04-02T16:07:17.775139Z"
    }
   },
   "outputs": [],
   "source": [
    "ff.create_distplot(\n",
    "    [data.loc[~data['malignant'], 'mean radius'],\n",
    "     data.loc[data['malignant'], 'mean radius']], \n",
    "    group_labels=[\"Benign\",\"Malignant\"],\n",
    "    bin_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Looking at the above histograms could you describe a rule to predict whether or a cell is malignant?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data Train-Test Split\n",
    "\n",
    "Always split your data into training and test groups.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:17.924717Z",
     "start_time": "2018-04-02T16:07:17.899036Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_tr, data_te = train_test_split(data, test_size=0.10, random_state=42)\n",
    "print(\"Training Data Size: \", len(data_tr))\n",
    "print(\"Test Data Size: \", len(data_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the `X` and `Y` matrices for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_tr[['mean radius']].to_numpy()\n",
    "Y = data_tr['malignant'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "## Using Least-Squares Regression\n",
    "\n",
    "_\"I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail.\"_ -  Abraham Maslow The Psychology of Science\n",
    "\n",
    "We would like to predict whether the tumor is malignant from the size of the tumor.  We have encoded whether a tumor is malignant or benign as 1 or 0.  Those are numbers that we could pretend are continuous and directly apply least squares regression.  Why not start there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"ComingSoon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we use scikit-learn to fit a least squares linear regression model.  Note, we will not use any regularization since this is a really simple one-dimensional model with a comparatively large training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.006048Z",
     "start_time": "2018-04-02T16:07:17.933377Z"
    }
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.026129Z",
     "start_time": "2018-04-02T16:07:18.007888Z"
    }
   },
   "outputs": [],
   "source": [
    "X_plt = np.expand_dims(np.linspace(X.min(), X.max(), 50),1)\n",
    "model_line = go.Scatter(name=\"Least Squares\",\n",
    "    x=X_plt.flatten(), y=lin_reg.predict(X_plt), \n",
    "    mode=\"lines\", line=dict(color=\"orange\"))\n",
    "go.Figure([points, model_line], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:\n",
    "\n",
    "1. Are we happy with the fit?\n",
    "2. What is the meaning of predictions that are neither 0 or 1?\n",
    "3. Could we use this to make a decision?\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Error\n",
    "\n",
    "How do we measure the error in our model?  In the following, we will examine some of the different error models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"ComingSoon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error\n",
    "\n",
    "In past lectures, we have used the root mean squared error as a measure of error.  We can compute that here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.032417Z",
     "start_time": "2018-04-02T16:07:18.028104Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "yhat = lin_reg.predict(X)\n",
    "print(\"Training RMSE:\", np.sqrt(mse(Y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does that mean for this data?  It is difficult to interpret this error in the context of a classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Error\n",
    "\n",
    "This is a classification problem so we probably want to measure how often we predict the correct value.  This is sometimes called the zero-one loss (or error):\n",
    "\n",
    "$$ \\large\n",
    "\\textbf{ZeroOneLoss} = \\frac{1}{n} \\sum_{i=1}^n \\textbf{I}\\left[ y_i \\neq f_\\theta(x_i) \\right]\n",
    "$$\n",
    "\n",
    "However to use the classification error we need to define a decision rule that maps $f_\\theta(x)$ to the $\\{0,1\\}$ classification values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Constant Threshold Decision Rule\n",
    "\n",
    "Suppose we instituted the following simple decision rule:\n",
    "\n",
    "$$\\Large\n",
    "\\text{If } f_\\theta(x) > 0.5  \\text{ predict 1 (malignant) else predict 0 (benign).}\n",
    "$$\n",
    "\n",
    "This simple **decision rule** is deciding that a tumor is malignant if our model predicts a values above 0.5 (closer to 1 than zero).\n",
    "\n",
    "In the following we plot the implication of these decisions on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_mal_hat = lin_reg.predict(X) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot we color the data points according to our decision rule and depict the decision boundary as a dotted vertical line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.059302Z",
     "start_time": "2018-04-02T16:07:18.034361Z"
    }
   },
   "outputs": [],
   "source": [
    "mal_points = go.Scatter(name=\"Classified as Malignant\", \n",
    "                        x=X[is_mal_hat].flatten(), y = jitter(Y[is_mal_hat]), \n",
    "                        mode=\"markers\", marker=dict(opacity=0.5, color=\"red\"))\n",
    "ben_points = go.Scatter(name=\"Classified as Benign\", \n",
    "                        x=X[~is_mal_hat].flatten(), y = jitter(Y[~is_mal_hat]),\n",
    "                        mode=\"markers\", marker=dict(opacity=0.5, color=\"blue\"))\n",
    "\n",
    "dec_boundary = (0.5 - lin_reg.intercept_)/lin_reg.coef_[0]\n",
    "dec_line = go.Scatter(name=\"Least Squares Decision Boundary\", \n",
    "                      x = [dec_boundary,dec_boundary], y=[-0.5,1.5], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash=\"dot\"))\n",
    "go.Figure([mal_points, ben_points, model_line,dec_line], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the `ZeroOneLoss`\n",
    "\n",
    "The zero-one loss is so commonly used that there is a built-in function for it in scikit-learn.  Here we compute the zero-one-loss for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.067202Z",
     "start_time": "2018-04-02T16:07:18.061723Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "\n",
    "print(\"Training Fraction incorrect:\", \n",
    "      zero_one_loss(Y, is_mal_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** \n",
    "\n",
    "1. Are we happy with this error level?\n",
    "1. What error would we get if we just guessed the label?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines for Comparison\n",
    "\n",
    "In any modeling task, when discussing the error it often helpful to have a baseline for comparison.  For example, in the earlier regression lectures, a reasonable baseline for comparison would be the constant model that just predicts the average value of $Y$.  \n",
    "\n",
    "For classification tasks, a reasonable baseline would be to predict the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.072065Z",
     "start_time": "2018-04-02T16:07:18.069177Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Fraction of Malignant Samples:\", Y.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if we guess the majority class **benign** we would get what accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.078040Z",
     "start_time": "2018-04-02T16:07:18.074158Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can figure this out from the above number\n",
    "print(\"Guess Majority:\",  zero_one_loss(Y, np.zeros(len(Y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, we get an error that is identical to the fraction of example in the other class (or classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is standard example of a common problem in classification (and perhaps modern society): **class imbalance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance\n",
    "\n",
    "\n",
    "Class imbalance is when a disproportionate fraction of the samples are in one class (in this case benign).  \n",
    "In extreme cases (e.g., fraud detection) only tiny fraction of the training data may contain examples in particular class.  In these settings we can achieve very high-accuracies by always predicting the frequent class without learning a good classifier for the rare classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Addressing Class Imbalance\n",
    "\n",
    "There are many techniques for managing class imbalance here are a few:\n",
    "\n",
    "1. Re-sample data to reduce or eliminate the class imbalance.\n",
    "2. Try learning algorithm that are a little more robust to class imbalance (e.g., decisions trees).\n",
    "3. Adjust the loss function to put a larger penalty on the smaller class.\n",
    "\n",
    "In this example the class imbalance is not that extreme so we will continue without re-sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "## Predicting a Probability\n",
    "\n",
    "\n",
    "Is the linear model predicting the \"probability\" of a tumor being malignant? Not really.  Probabilities are constrained between 0 and 1.   How could we learn a model that captures this probabilistic interpretation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"ComingSoon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Constraining the Line \n",
    "\n",
    "At the very least our model should probably predict a number between zero and one.  This would at least be closer to being a probability.  We could try to constrain the model:\n",
    "\n",
    "$$ \\large\n",
    "p_i = \\min\\left(\\max \\left( x^T \\theta , 0 \\right), 1\\right)\n",
    "$$\n",
    "\n",
    "this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.131948Z",
     "start_time": "2018-04-02T16:07:18.128385Z"
    }
   },
   "outputs": [],
   "source": [
    "def bound01(z):\n",
    "    u = np.where(z > 1, 1, z)\n",
    "    return np.where(u < 0, 0, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.155130Z",
     "start_time": "2018-04-02T16:07:18.134002Z"
    }
   },
   "outputs": [],
   "source": [
    "p_line = go.Scatter(name=\"Truncated Least Squares\",\n",
    "    x=X_plt.flatten(), y=bound01(lin_reg.predict(X_plt)), \n",
    "    mode=\"lines\", line=dict(color=\"green\", width=8))\n",
    "py.iplot([mal_points, ben_points, model_line, p_line, dec_line], filename=\"lr-06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far least squares regression seems pretty reasonable and we can \"force\" the predicted values to be bounded between 0 and 1.  \n",
    "\n",
    "**Can we interpret the truncated values as probabilities?** \n",
    "\n",
    "Perhaps, but it would depend on how the model is estimated (more on this soon).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "# An Issue with Extreme Points \n",
    "\n",
    "It seems like large tumor sizes are indicative of malignant tumors.  Suppose we observed a very large malignant tumor that is 100mm in mean radius.  What would this do to our model?\n",
    "\n",
    "\n",
    "Let's add an extra data point and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:07:18.165906Z",
     "start_time": "2018-04-02T16:07:18.157445Z"
    }
   },
   "outputs": [],
   "source": [
    "X_ex = np.vstack([X, [100]])\n",
    "Y_ex = np.hstack([Y, 1.])\n",
    "lin_reg_ex = LinearRegression()\n",
    "lin_reg_ex.fit(X_ex, Y_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:08:52.741027Z",
     "start_time": "2018-04-02T16:08:52.706346Z"
    }
   },
   "outputs": [],
   "source": [
    "extreme_point = go.Scatter(\n",
    "    name=\"Extreme Point\", x=[100], y=[1], mode=\"markers\", \n",
    "    marker=dict(color=\"green\", size=10))\n",
    "model_line.line.color = \"gray\"\n",
    "\n",
    "\n",
    "X_plt_ex = np.expand_dims(np.linspace(np.min(X)-5, np.max(X)+5, 100),1)\n",
    "model_line_ex = go.Scatter(name=\"New Least Squares\",\n",
    "                           x=X_plt.flatten(), y=lin_reg_ex.predict(X_plt_ex), \n",
    "                           mode=\"lines\", line=dict(color=\"orange\"))\n",
    "\n",
    "dec_line.line.color = \"gray\"\n",
    "dec_boundary_ex = (0.5 - lin_reg_ex.intercept_)/lin_reg_ex.coef_[0]\n",
    "dec_line_ex = go.Scatter(\n",
    "    name=\"Decision Boundary\", \n",
    "    x = [dec_boundary_ex, dec_boundary_ex], y=[-0.5,1.5], mode=\"lines\",\n",
    "    line=dict(color=\"black\", dash=\"dash\"))\n",
    "\n",
    "\n",
    "\n",
    "go.Figure([mal_points, ben_points,model_line, model_line_ex, dec_line, dec_line_ex, extreme_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of the extreme point shifted the linear model from the gray Least Squares line to the new Orange Least squares line.  This shift actually moved the decision boundary and produced a **less accurate** model!  This is a little surprising.  Indeed, if we keep increasing the size of this one tumor we can cause the model to incorrectly classify all the smaller malignant tumors as benign.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T16:09:16.748183Z",
     "start_time": "2018-04-02T16:09:16.743636Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Before:\", \n",
    "      zero_one_loss(Y_ex, lin_reg.predict(X_ex) > 0.5))\n",
    "print(\"After:\", \n",
    "      zero_one_loss(Y_ex, lin_reg_ex.predict(X_ex) > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this problem, we need to both adjust our model and also introduce a loss function that is more appropriate for the classification task.  In the next notebook, we introduce the logistic regression model and negative log-likelihood (cross entropy) loss. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
